% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{russakovsky2015imagenet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein \emph{et~al.}, ``Imagenet large scale
  visual recognition challenge,'' \emph{International Journal of Computer
  Vision}, vol. 115, 2015.

\bibitem{han2016mcdnn}
S.~Han, H.~Shen, M.~Philipose, S.~Agarwal, A.~Wolman, and A.~Krishnamurthy,
  ``Mcdnn: An approximation-based execution framework for deep stream
  processing under resource constraints,'' in \emph{Proceedings of the 14th
  Annual International Conference on Mobile Systems, Applications, and
  Services}.\hskip 1em plus 0.5em minus 0.4em\relax ACM, 2016.

\bibitem{shen2016fast}
H.~Shen, S.~Han, M.~Philipose, and A.~Krishnamurthy, ``Fast video
  classification via adaptive cascading of deep models,'' \emph{arXiv preprint
  arXiv:1611.06453}, 2016.

\bibitem{kang2017noscope}
D.~Kang, J.~Emmons, F.~Abuzaid, P.~Bailis, and M.~Zaharia, ``Noscope:
  optimizing neural network queries over video at scale,'' \emph{Proceedings of
  the VLDB Endowment}, vol.~10, 2017.

\bibitem{shen2017fast}
H.~Shen, S.~Han, M.~Philipose, and A.~Krishnamurthy, ``Fast video
  classification via adaptive cascading of deep models,'' \emph{arXiv
  preprint}, 2017.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton, ``Learning multiple layers of features from tiny
  images,'' \emph{Technical Report}, 2009.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei, ``Imagenet: A
  large-scale hierarchical image database,'' in \emph{Computer Vision and
  Pattern Recognition, 2009. CVPR 2009. IEEE Conference on}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2009.

\bibitem{imagenet_cvpr09}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei, ``{ImageNet: A
  Large-Scale Hierarchical Image Database},'' in \emph{CVPR09}, 2009.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for
  large-scale image recognition,'' \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean, ``Distilling the knowledge in a neural
  network,'' \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{fu2017look}
J.~Fu, H.~Zheng, and T.~Mei, ``Look closer to see better: Recurrent attention
  convolutional neural network for fine-grained image recognition,'' in
  \emph{Conf. on Computer Vision and Pattern Recognition}, 2017.

\bibitem{razavian2014cnn}
A.~S. Razavian, H.~Azizpour, J.~Sullivan, and S.~Carlsson, ``Cnn features
  off-the-shelf: an astounding baseline for recognition,'' in \emph{Computer
  Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference
  on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2014.

\bibitem{zhuang2015supervised}
F.~Zhuang, X.~Cheng, P.~Luo, S.~J. Pan, and Q.~He, ``Supervised representation
  learning: Transfer learning with deep autoencoders.'' in \emph{IJCAI}, 2015.

\bibitem{sun2016return}
B.~Sun, J.~Feng, and K.~Saenko, ``Return of frustratingly easy domain
  adaptation.'' in \emph{AAAI}, vol.~6, no.~7, 2016.

\bibitem{huang2017densely}
G.~Huang, Z.~Liu, K.~Q. Weinberger, and L.~van~der Maaten, ``Densely connected
  convolutional networks,'' in \emph{Proceedings of the IEEE conference on
  computer vision and pattern recognition}, vol.~1, no.~2, 2017.

\bibitem{ba2014deep}
J.~Ba and R.~Caruana, ``Do deep nets really need to be deep?'' in
  \emph{Advances in neural information processing systems}, 2014.

\bibitem{dauphin2013big}
Y.~N. Dauphin and Y.~Bengio, ``Big neural networks waste capacity,''
  \emph{arXiv preprint arXiv:1301.3583}, 2013.

\bibitem{chen2017learning}
G.~Chen, W.~Choi, X.~Yu, T.~Han, and M.~Chandraker, ``Learning efficient object
  detection models with knowledge distillation,'' in \emph{Advances in Neural
  Information Processing Systems}, 2017.

\bibitem{lopez2015unifying}
D.~Lopez-Paz, L.~Bottou, B.~Sch{\"o}lkopf, and V.~Vapnik, ``Unifying
  distillation and privileged information,'' \emph{arXiv preprint
  arXiv:1511.03643}, 2015.

\bibitem{kim2015compression}
Y.-D. Kim, E.~Park, S.~Yoo, T.~Choi, L.~Yang, and D.~Shin, ``Compression of
  deep convolutional neural networks for fast and low power mobile
  applications,'' \emph{arXiv preprint arXiv:1511.06530}, 2015.

\bibitem{bucilu2006model}
C.~Bucilu«é, R.~Caruana, and A.~Niculescu-Mizil, ``Model compression,'' in
  \emph{Proceedings of the 12th ACM SIGKDD international conference on
  Knowledge discovery and data mining}.\hskip 1em plus 0.5em minus 0.4em\relax
  ACM, 2006.

\bibitem{li2015towards}
C.~Li, Y.~Hu, L.~Liu, J.~Gu, M.~Song, X.~Liang, J.~Yuan, and T.~Li, ``Towards
  sustainable in-situ server systems in the big data era,'' in \emph{ACM
  SIGARCH Computer Architecture News}, vol.~43, no.~3.\hskip 1em plus 0.5em
  minus 0.4em\relax ACM, 2015.

\bibitem{jaderberg2014speeding}
M.~Jaderberg, A.~Vedaldi, and A.~Zisserman, ``Speeding up convolutional neural
  networks with low rank expansions,'' \emph{arXiv preprint arXiv:1405.3866},
  2014.

\bibitem{romero2014fitnets}
A.~Romero, N.~Ballas, S.~E. Kahou, A.~Chassang, C.~Gatta, and Y.~Bengio,
  ``Fitnets: Hints for thin deep nets,'' \emph{arXiv preprint arXiv:1412.6550},
  2014.

\bibitem{xue2014singular}
J.~Xue, J.~Li, D.~Yu, M.~Seltzer, and Y.~Gong, ``Singular value decomposition
  based low-footprint speaker adaptation and personalization for deep neural
  network,'' in \emph{Acoustics, Speech and Signal Processing (ICASSP), 2014
  IEEE International Conference on}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2014.

\bibitem{chen2015compressing}
W.~Chen, J.~Wilson, S.~Tyree, K.~Weinberger, and Y.~Chen, ``Compressing neural
  networks with the hashing trick,'' in \emph{International Conference on
  Machine Learning}, 2015.

\bibitem{han2015learning}
S.~Han, J.~Pool, J.~Tran, and W.~Dally, ``Learning both weights and connections
  for efficient neural network,'' in \emph{Advances in neural information
  processing systems}, 2015.

\bibitem{teerapittayanon2016branchynet}
S.~Teerapittayanon, B.~McDanel, and H.~Kung, ``Branchynet: Fast inference via
  early exiting from deep neural networks,'' in \emph{Pattern Recognition
  (ICPR), 2016 23rd International Conference on}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2016.

\bibitem{panda2016conditional}
P.~Panda, A.~Sengupta, and K.~Roy, ``Conditional deep learning for
  energy-efficient and enhanced pattern recognition,'' in \emph{Design,
  Automation \& Test @article{jaderberg2014speeding, title={Speeding up
  convolutional neural networks with low rank expansions}, author={Jaderberg,
  Max and Vedaldi, Andrea and Zisserman, Andrew}, journal={arXiv preprint
  arXiv:1405.3866}, year={2014} }in Europe Conference \& Exhibition (DATE),
  2016}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2016.

\end{thebibliography}
