\documentclass{article}
\input{header}
\usepackage{nips_2018}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{xcolor}

\title{Formatting instructions for NIPS 2018}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle


\begin{abstract}
  We can meet thousands of objects through years, i.e., different kind of animals, plants, and various people, which requires a general model classifying all these object. On the other hand, in a specific environment, like our home or office, only less than ten objects appear, such as family members, pets, and furniture. Thus a special model recognizing a few objects could be sufficient. Using a special model in these environment could lead automatically to a much higher accuracy since there is a reduction in number of classes. However, using special models has a intrinsic problem that there are so many environment and mind-bogglingly many combinations of objects, which makes it infeasible to train special models one-by-one. In the contrast, the general model could be trained once and used everywhere. General model also has another benefit that it has seen much more images than a special model and a special model derived from the general model could have much higher accuracy than the one trained from scratch. The problem is, given various combination of classes and distribution, could we train a general model and \textit{derive specialized models from it efficiently for all combinations?} The intuitive idea would be transfer learning, in which the last few layers of the general model will be retrained for the specialized dataset. However, considering the time latency and energy consuming of retraining a model, we argue that avoiding retraining is critical in the efficient derivation of specialied models. Instead, we brought up \textit{probability layer}, which could adapt the general model to all combinations of classes and distribution efficiently without any retraining. Through experiments, it was shown that probability layer without retraining could perform equivalent or even better than transfer learning with retraining.
\end{abstract}


\section{Introduction}
% First paragraph: CNN performs very well at lab setting
Convolutional neural network (CNN) plays an important role in image recognition. In 2012, AlexNet \cite{krizhevsky2012imagenet} achieved a top-5 error of 17\% on ImageNet \cite{deng2009imagenet}, while previous method could only achieve a top-5 error of 25.7\%. Since then, CNNs have become the dominant method and main research direction in image recognition. In 2015, ResNet \cite{he2016deep} achieves a top-5 error of 3.57\%. Considering that the estimated human classification error on ImageNet is 5.1\% \cite{russakovsky2015imagenet}, we can conclude that CNNs have gained the ability of performing better than human. 



% Second paragraph: All of these models are targetting thousands of classes. In a special environment, we only confront less objects.
However, all of these results are gained in lab setting, which overlooks the environment information that we can use in real life. According to our experience, the appearance of classes has strong time and spatial locality. Even if we can meet thousands of objects through years, only less than ten objects would appear in a particular environment, like our home or office. Experiments on videos of day-to-day life from Youtube \cite{shen2017fast} shows that $10$ objects comprised $90$\% of all objects in $85$\% time. While we can train a general model classifying thousands of classes in lab, a special model for tens of objects would be sufficient. This reduction in number of classes could lead to a boost in accuracy automatically. For example, if we randomly guess from $1000$ classes, the accuracy is $0.1\%$, while the accuracy for randomly guess from $10$ classes is $10\%$. In other word, even if we have changed nothing in the model architecture, the reduction in number of classes could naturally provide us a higher accuracy. Thus it is desirable to incorporate the environment information and use a specialized model instead of a general one. The obstacle to this fascinating plan is the mind-bogglingly huge number of specialized scenarios. Considering the combinations of classes, if we take $10$ classes out of $100$ classes, there would be $1.73*10^{14}$ combinations. If class distribution is also taken into consideration, this number could increase further. In order to use specialized models, we much know how to get them efficiently on various combinations of classes and different class distributions. 
% Third paragraph: Transfer learning becomes the dominant method. 
Transfer learning is the dominant method in conquering various problems for implementing CNNs in real world. In transfer learning, we will have a source domain and a target domain. A model will be pretrained on the source domain and the last few layers will be retrained on the target domain such that the model can handle testing data from the target domain. In this process, various problems could be solved, such as lacking training data, using unlabeled data \cite{doersch2015unsupervised, noroozi2016unsupervised}, and allowing quick runtime domain adaption \cite{han2016mcdnn, shen2017fast}. Transfer learning has also shown improvement on accuracy compared with training a model from scratch \cite{oquab2014learning, yosinski2014transferable}, since the model has seen more images in the process of transfer learning. Due to the effectiveness in all these domains, transfer learning has become the method off the top of head. To the best of our knowledge, transfer learning is the only method used to utilize the scenario we described.


% Forth paragraph: Transfer learning needs retraining and shares all problems in training a model from scratch.
Although transfer learning can bring in many benefits, it also has requires expertise in training models and has intrinsic obstacle in automatic retraining. First, the general practice in transfer learning is to freeze the parameters in leading layers and retrain the last few layers, while existing papers \cite{yosinski2014transferable} show that freezing leading layers may have strong negative effect on the accuracy. Without testing various settings, it is hard to know how many layers should be frozen. Second, the choice of training step depends on dataset and a unique setting is hard to find, which makes automatical transfer learning very hard, if not impossible. For example, in the training process of VggNet \cite{simonyan2014very}, a shallow model needs to be trained to initialize the weights in a deeper model and the learning rate is also carefully selected for several times. There is also a long-lasting debate for the best choice of hyper-parameters for various architectures and datasets \cite{bergstra2012random,  glorot2010understanding, he2015delving, smith2017don, wilson2003general}. All of these evidence shows that retraining a model is not a straight forward task and expertise in training process is required, which obstacle the automatical retraining. Third, due to the short time for retraining, the model may not converge and lead to dramatical variance in accuracy, even with the same model and dataset. Given all these intrinsic problems to implement transfer learning in the scenario, a natural question would be whether we can find a method to use the scenario \textit{without retraining}? 

% Fifth paragraph: Rescale has good effect and can replace transfer learning in a scenario.
To use the scenario without retraining, we brought up \textit{probability layer} which can avoid retraining totally and perform better than transfer learning. The benefit are three folds. First, probability layer can make use of the scenario without any retraining, which would save lots of time. Second, probability layer can make prediction based on the knowledge of original model and performs better than training from scratch. Third, through series of experiments, probability layer is shown to have equivalent or even better performance than transfer learning. We argue that probability layer is a better method than transfer learning to make use of various kind of scenarios. 

% Sixth paragraph: Analysis on logits makes our paper different from rescaling
To use probability layer efficiently, the choice of general model is critical and a detailed analysis of logits could predict whether the probability layer will perform well on a general model. Logits is the output of softmax layer which is equipped with almost all the CNNs. Traditional usage of logits is to consider the label with highest logits as the predicted label. Recent works \cite{ba2014deep, bucilua2006model, hinton2015distilling, lopez2015unifying} have shown that logits contain more information beyond that. Actually, logits is critical to the success of probability layer. Through analysis on logits, the selection criteria of the general model is provided and we can predict whether probability layer will work well on a general model without actually running the probability layer.

\section{Related Works}
% Talk about scenario
Many papers have talked about the difference between testing CNN models in lab setting and in the wild. In lab setting, the prevalent benchmarks, like ImageNet \cite{deng2009imagenet} and CIFAR100 \cite{krizhevsky2009learning}, all assumes equal number of images for each class. However, in a more realistic setting \cite{han2016mcdnn, shen2017fast}, different class may have dramatically different frequency and the distribution may also change over time. The question we want to answer is that, given an environment with a number of classes and distribution, how to use this information? All of the existing works \cite{han2016mcdnn, shen2017fast} only use transfer learning to retrain the last few layers, which would introduce huge amount of energy-consumption and time latency. Our method will make use of the distribution efficiently without retraining, thus avoid additional energy-consumption and time latency.

% Unbalanced dataset
Unbalanced dataset focus on the problem that training dataset has different distribution with the testing dataset, which is related to our scenario. One key point of unbalanced dataset is that the minor classes in the training dataset cannot gain same attention as the major classes, thus oversampling on minor classes or undersampling on major classes will be implemented to generate a balanced dataset \cite{wang2016dealing}. Our scenario is different since we assume that we have a model pretrained on balanced dataset and in the testing time the class distribution changes. Another key point of unbalanced dataset is that in the running time, as the distribution changes, the model will keep updating as new inputs come and retraining is constantly needed. Our method avoid retrain at all and could achieve the same or better performance by only changing a few parameters according to the distribution automatically. Thus energy efficiency and speedup could be achieved by our solution.


% Talk about transfer learning
Transfer learning has shown benefit in domain adaption and currently is the dominant method, if not the only one, for domain adaption. To solve the problem that the testing dataset is small, we use transfer learning \cite{oquab2014learning}. To use unsupervised dataset \cite{doersch2015unsupervised, noroozi2016unsupervised}, we use transfer learning. Assume we have a large model handling $1000$ classes and in an environment only $10$ classes appearing, we still use transfer learning \cite{han2016mcdnn, shen2017fast}. Transfer learning has become the only method off the top of head when we consider the change in classes. Although transfer learning shows various benefit, it also has intrinsic shortage residing in the process of training a CNN model. First, it is hard to decide how many layers we should freeze. \cite{yosinski2014transferable} reported that the freezing less layers leads to better performance on different domains and freezing more layers lead to a better performance on similar domains since the co-adapted interactions between layers will be kept. However, it is still hard to decide whether two domains are similar enough and the exact effect of freezing various number of layers. Second, it is hard to decide the hyper-parameters unless actually try different settings. For epoch number, it is hard to predict whether the model will converge or collapse after a fixed number of epochs. The choice of learning rate also depends on both model and dataset. The choice of hyper-parameter need expertise in lab to fine-tune, which is not convenient in a automatical product. Third, the long latency and energy consuming of training a model obstacle the transfer learning on energy-efficient devices, especially in an environment that class number and distributions keep changing. Our method can avoid retraining at all while utilizing the new dataset, thus all the inconvenience related to retraining is avoided naturally.

\section{Probability Layer}
In this section, we introduce the probability layer and the PCNN. Before proceeding further, we introduce the notation that will be used in the rest of the paper.

\textbf{Notation.} A CNN takes as input an image $X$ and outputs a vector of estimated probability for each class \begin{equation}
    \vec{p} = (p_1, p_2, ..., p_n)
\end{equation}
, where $n$ is the number of classes, and select the class with highest estimated probability 
\begin{equation}
    \underset{i}{\text{argmax}} \; p_i
\end{equation}
as the true class. Here we treat the whole CNN model as a classifier which gives the estimated probability $p_i$ for the label i given the input image X
\begin{equation}
    p_i = P(i|X)
\end{equation}. After adding the probability layer, the original prediction $P(i|X)$ will be rescaled and we use $P_t(i|X)$ to denote the rescaled probability. Additionally, we define $P(i)$ and $P_t(i)$ for class distribution in training data and testing data respectively.


\textbf{Key Assumption. } 
% Key assumption
The main difference between proposed layer and the original CNNs is that we take into consideration the environment information. In original CNNs, the prediction for each image will be made individually, assuming a sequence of images is independent and identically distributed (\textit{i.i.d}). However, in real life, this assumption does not hold and strong spatial and temporal locality may exist. In particular, we assume that in the environment the number of classes, class type, and class distribution are fixed while huge difference between training dataset and testing dataset exists. This is a reasonable assumption on environment in real life. Experiments on videos of day-to-day life from Youtube \cite{shen2017fast} shows that $10$ objects comprised $90$\% of all objects in $85$\% time. 

%The prediction will be made by CNNs individually, assuming a sequence of images is independent and identically distributed (i.i.d). In real life, this assumption does not hold and strong spatial and temporal

In this section, we present the \textit{probability layer}. Probability layer is an extra layer after the CNN model, rescaling the output of softmax layer. Rescaling is a topic in statistics \cite{saerens2002adjusting}. To the best of our knowledge, we are the first to discuss rescaling in CNN context. The outputs of original CNNs predict the probability for each class and the probability layer will adjust this prediction based on the difference of class distribution in training and testing dataset. In particular, for classes with different distribution in training and testing dataset, the probability layer will rescale the corresponding outputs from softmax layer according to the difference in distribution. For other classes with same distribution in both training and testing dataset, the outputs of the proposed layer are equal to the outputs of the softmax layer. 


The probability layer will take as inputs the original predicted probability, class distribution in training dataset, as well as the distribution in testing dataset, and output a vector for the rescaled prediction. The first input is the prediction $\vec{P(\cdot|X)}$ from the softmax layer, which represents the original predicted probability for each class from the original CNNs. The second input is a vector of class distribution $\vec{P(\cdot)}$ in training dataset and the third one is a vector of class distribution $\vec{P_t(\cdot)}$ in testing dataset. The probability layer will rescale the predicted probability in $\vec{P(\cdot|X)}$ elementwisely and produce as output a vector $\vec{P_t(\cdot|X)}$ for the rescaled prediction of each class.

Formally, let the outputs of CNNs with and without rescaling are
\begin{equation}
    P_t(i|X) = \frac{P_t(X|i) \cdot P_t(i)}{P_t(x)}
\end{equation}
and
\begin{equation}
    P(i|X) = \frac{P(X|i) \cdot P(i)}{P(x)}
\end{equation}
respectively. Here $P_t(i)$ means the class distribution in testing dataset and $P_t(i|X)$ represents the predicted probability for class $i$ after the probability layer. We assume that $P_t(X|i)$ equals $P(X|i)$ approximately, where $P(X|i)$ is the distribution of image data for class $i$. This assumption makes sense since, for a class $i$, the selection of input x is random. Transforming equation $4$ and $5$ into 
\begin{equation}
    P_t(X|i) = \frac{P_t(i|X) \cdot P_t(X)}{P_t(i)}
\end{equation}
and
\begin{equation}
    P(X|i) = \frac{P(i|X) \cdot P(X)}{P(i)}
\end{equation}
and following our assumption that $P_t(X|i) = P(X|i)$, we can derive that 
\begin{equation}
    P_t(i|X) = \frac{P_t(i)}{P(i)}\cdot P(i|X) \cdot P(X) 
\end{equation}. Considering $\sum_{i=1}^n P_t(i|X) = 1$, we can get that $P(X) = 1/(\sum_{j=1}^{n} \frac{P_t(j)}{P(j)} \cdot P(j|X))$. Finally, the rescaling formular is 
\begin{equation}
    P_t(i|X) = \frac{\frac{P_t(i)}{P(i)} \cdot P(i|X)}{\sum_{j=1}^n \frac{P_t(i)}{P(j)} \cdot P(j|X)}
\end{equation}


To give probability layer the ability of detecting new classes, we choose not to rescale the outputs from softmax layer when original model has strong confidence in its prediction and set the formular of probability layer as
\begin{equation}
    P_t(i|X) = \frac{\frac{P_t(i)}{P(i)} \cdot P(i|X)}{\sum_{j=1}^n \frac{P_t(i)}{P(j)} \cdot P(j|X)} \cdot I_{\{P(i|X) < \omega\}} + P(i|X) \cdot I_{\{P(i|X) >= \omega\}}
\end{equation}
, where $\omega$ is the threshold when we should trust the original prediction and $I_X$ is the indicator function such that
\begin{equation}
I_X(x) = \begin{cases}
1, &\text{if $x \in X$}\\
0, &\text{$o.w.$}
\end{cases}
\end{equation}
. If a model have a strong confidence in its prediction, the accuracy would be much higher than the model's average accuracy. Fig. \ref{fig:threshold} shows that accuracy on denseNet for images with predicted probability higher than various threshold. While the original denseNet has top-1 accuracy as $73\%$, the accuracy increase to $83\%$ when we set the threshold $\omega$ to be $75\%$. When we increase furtherly the threshold $\omega$ to be $99\%$, the accuracy would increase to $95.01\%$. Thus when a model has strong confidence in its prediction, we should better believe in the model instead of rescaling. The bar plot in Fig. \ref{fig:threshold} also shows that lots of images can get high predicted probability from original model. For example, in denseNet, there are more than 60\% images get predicted probability higher than 95\%. Thus most of the images from minor classes could be classified correctly. 

\begin{figure}
\centering
\includegraphics[scale = 0.6]{threshold.png}
\caption{Probability Layer Intuition}
\label{fig:threshold}
\end{figure}




\begin{figure}
\begin{subfigure}{.25\textwidth}
  \centering
  \includegraphics[width=\textwidth]{rocket.png}
  \caption{Rocket}
  \label{fig:sub2}
\end{subfigure}
\begin{subfigure}{.25\textwidth}
  \centering
  \includegraphics[width=\textwidth]{bottle.png}
  \caption{Bottle}
  \label{fig:sub3}
\end{subfigure}
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=\textwidth]{intuition.png}
  \caption{Computation Intuition}
  \label{fig:sub1}
\end{subfigure}

\caption{Probability Layer Intuition}
\label{fig:intuition}
\end{figure}


% Intuition of probability layer
Probability layer helps by using environment information that original CNNs does not use. When human recognizes an object, both vision and environment information will be used, i.e., what we have seen recently and which objects may appear here. However, CNNs can only make use of vision information while discarding environment information, which makes it extremely difficult to distinguish classes with similar shapes. For example, fig. \ref{fig:sub2} and \ref{fig:sub3} shows images from CIFAR100 for bottle and rocket respectively. It is hard to distinguish these two classes only from images while environment information can easily rule out rocket in most scenarios. 

Fig. \ref{fig:intuition} gives intuition on how probability layer utilize environment information. In fig. \ref{fig:sub1}, the lower row represents the outputs from softmax layer and the upper row represents the probability layer. The orange nodes stand for the classes with high predicted probability in softmax layer and the red nodes stand for the suggestion from environment. The prediction from probability layer will be selected from the intersection of the set of red nodes and orange nodes, which rules out confusing classes for CNNs. 
%how to select general model?


% compare probability layer with naive mask

\section{Experiments}
To show their potential, we evaluate PCNN on three tasks. First, with fixed distribution, we compare the performance of probability layer with transfer learning. Second, for data stream with minor classes, we compare the performance of probability layer with transfer learning. Third, we show that probability layer is complementary to acceleration methods through combining probability layer with spatial redundancy elimination approach \cite{figurnov2016perforatedcnns} to achieve decreased computation and improved accuracy.

\subsection{Probability layer with fixed distribution}
% Third paragraph: Experiment setting
We evaluate probability layer's ability in making use of environment on number of classes as well as in-class distribution when number of classes is fixed. The dataset is a subset of CIFAR100 \cite{krizhevsky2009learning}. CIFAR100 \cite{krizhevsky2009learning} is a dataset containing 100 classes with $50,000$ images in training dataset and $10,000$ images in testing dataset. We trained denseNet \cite{huang2017densely} on original CIFAR100 training datasets. For testing part, to evaluate the performance of probability layer under various conditions, we choose a subset of testing images as the new testing dataset. For various number of classes, we select a subset of classes and each class has equal number of images. For testing the ability to adapt to a dataset with a few major classes and many minor classes, we choose $10$ classes as the major class, select all $100$ testing images for each class, and randomly choose a fixed number of testing images in other $90$ classes, according to the desired percentage that $10$ major classes occupy. For implementing probability layer, we calculate the percentage of every class in the generated testing dataset and set up the parameters in probability layer correspondingly. When implementing transfer learning, we use the same model trained on CIFAR100 and retrain on the same generated training dataset. For the retraining process, we retrain all fully-connected layers after convolutional layers, since this is the method reported to have best accuracy gain (CITATIONS). In the retraining process, various epoches were used, i.e. $1$, $5$, $30$. The maximum epoch tried is $30$ because we consider both latency and energy which could be used in normal operation instead of retraining. For example, if the generated training dataset has 10 classes and 500 images in each class, retraining for $30$ epochs means that $150,000$ images will be processed by the models. Since generally the image will be processed every one second, the machine can proceed around $41$ more hours without so many retraining.

% Forth paragraph: Show probability layer works on complex models
Fig. \ref{fig:complexNumber} shows the comparison between probability layer and transfer learning on various number of classes. Original model represents the model with neither probability layer nor transfer learning, i.e. without adaption to the new dataset. We can find that the model with probability layer performs consistently better than the model without probability layer. When number of classes is less than $10$, probability layer can boost the top-1 accuracy to be more than 95\%, which is better than the human's ability in recognition, reported to be a top-5 error of 5.1\% \cite{russakovsky2015imagenet}. When number of classes is less than 10, probability layer's performance is at most 2\% lower than retraining for $5$ rounds. As number of classes becomes larger than $10$, probability layer would always perform better than retraining for $5$ rounds. The advantage of probability layer over retraining increases as number of classes increases. One point worth noting is that when the number of classes increases over $90$, retraining would bring worse accuracy than original model without retraining. In contrast, the probability layer can still bring 2\% advantage over original model. All these observations indicate that probability layer has better ability in using various environment, especially when the number of classes gets larger.

Fig. \ref{fig:complexNumber} shows the accuracy on testing data when there are $10$ major classes occupying various weights among $100$ classes. The other $90$ classes are minor classes with same weights. While the weights increase from $50$\% to $100$\%, probability layer performs consistently better than the retraining for $5$ or $1$ rounds. Even if we retrain the model for $30$ rounds, probability layer still performs better when weights of $10$ major classes is less than $85$\%. When the weights increase over $85$\%, the advantage of retraining for $30$ rounds over probability layer is at most $0.5$\%. Retraining for $30$ rounds takes $60$ seconds and xxx GFLOPS. Considering the intensive energy-consumption and time latency of retraining for $30$ rounds, this benefit of $0.5$\% advantage is not so significant. We should also note that retraining for $30$ rounds would make the accuracy to be much lower than original model when weights of major classes is less than 75\%. Since we need to choose a method before we start in automatically using environment information, we can conclude that probability layer is the most suitable method.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{PLvsRetrain.png}
        \caption{Various number of classes}
        \label{fig:complexNumber}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{VaryingDistribution.png}
        \caption{Ten classes occupying various percentage}
        \label{fig:complexDistribution}
    \end{subfigure}
    \caption{Probability layer on denseNet}\label{fig:complex}
\end{figure}


% Fifth paragraph: probability layer can boost accuracy to top-5 accuracy. TODO

\subsection{Probability layer with varied distribution}



\subsection{Combining acceleration methods}
A promising way to achieve high speedup and accuracy is to combine acceleration methods with probability layer. For this to succeed, the acceleration methods should utilize different types of redundancy in the network. In this section, we verify that probability layer can be combined with a acceleration methods of using spatial redundancy, \textit{PerforatedCNN} \cite{figurnov2016perforatedcnns}, to achieve high speed up while increasing top-1 accuracy.

We reimplemented the PerforatedCNN \cite{figurnov2016perforatedcnns} on denseNet. PerforatedCNN makes use of spatial redundancy in image by skipping evaluation in some of the spatial positions. Different from other methods in using spatial redundancy, i.e., increasing strides, PerforatedCNN will interpolate these skipped positions using nearest neighborhood, such that the output size will be unchanged. In this way, the architecture remains same and no retraining is needed. The shortage of PerforatedCNN is a huge decrease in accuracy, which could be made up by probability layer.

We first apply the probability layer to the network. Then we apply the spatial redundancy elimination methods to this network. In the whole process, no retraining is needed. The PerforatedCNN is tested at the theoretical speedup level of 2x. The testing dataset contains $5$ classes with equal frequency. The results are presented in the table xx. The PerforatedCNN decreases computation and accuracy while probability layer increase accuracy and has no contribution towards decreasing computation. Probability layer complements spatial redundancy elimination methods perfectly and the combined method achieves both increase in accurarcy and decrease in computation.

\begin{center}
\begin{tabular}{ c|c|c } 
 Method & Mult. $\downarrow$ & Accuracy $\uparrow$ (\%) \\ 
 \hline
 Probability Layer & 1.0x & 21\% \\ 
 Perforation & 2.0x & \\ 
 \hline
 Combined Method & - & -
\end{tabular}
\end{center}


\section{Conclusion}
Transfer learning has numerous intrinsic problems for using environment automatically. When the target classes does not change, probability layer is the alternative way to use environment without retraining and shows comparable or even better performance than transfer learning. Through analysis on logits, a metric is given such that, without actually running, we can decide whether probability layer can bring benefit to a model. Further, probability layer will not obstacle the updating on environment information and allow new classes to come when the model is pretty sure.



\bibliography{nips_2018}
\bibliographystyle{plain}


\end{document}
